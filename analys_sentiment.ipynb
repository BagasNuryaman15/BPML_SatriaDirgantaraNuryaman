{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Libarary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulasi Data\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "\n",
    "# Visualisasi Data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# NLP & Preprocessing\n",
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "# Machine Learning & Sentiment Analysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bagasnuryaman/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/bagasnuryaman/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bagasnuryaman/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/bagasnuryaman/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK Resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loading Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewId</th>\n",
       "      <th>userName</th>\n",
       "      <th>userImage</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>thumbsUpCount</th>\n",
       "      <th>reviewCreatedVersion</th>\n",
       "      <th>at</th>\n",
       "      <th>replyContent</th>\n",
       "      <th>repliedAt</th>\n",
       "      <th>appVersion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>169fd249-9e14-45b3-9fa5-b004f8792271</td>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>sedih bgt njir, akun aku hilang ud level 40an</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.109.1</td>\n",
       "      <td>2025-05-20 21:30:40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.109.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a59942c7-c9b3-4ffd-a026-3589cbb5be3c</td>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>game bayak buk bintang 1 Update terus buk nya ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.109.1</td>\n",
       "      <td>2025-05-20 21:29:45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.109.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1648cc00-59f2-4dcf-ac05-b974911f3aba</td>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>baguss gamenya</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-05-20 21:22:48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b3e7444a-f50c-415a-b030-2eff43d394c9</td>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>ehh min lu klo ngasih tim yg ngotak apaa masa ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.109.1</td>\n",
       "      <td>2025-05-20 21:22:35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.109.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66863758-112b-4c94-b583-47b7306f0a6c</td>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>bagus banget game nya</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-05-20 21:19:35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               reviewId         userName  \\\n",
       "0  169fd249-9e14-45b3-9fa5-b004f8792271  Pengguna Google   \n",
       "1  a59942c7-c9b3-4ffd-a026-3589cbb5be3c  Pengguna Google   \n",
       "2  1648cc00-59f2-4dcf-ac05-b974911f3aba  Pengguna Google   \n",
       "3  b3e7444a-f50c-415a-b030-2eff43d394c9  Pengguna Google   \n",
       "4  66863758-112b-4c94-b583-47b7306f0a6c  Pengguna Google   \n",
       "\n",
       "                                           userImage  \\\n",
       "0  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "1  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "2  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "3  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "4  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "\n",
       "                                             content  score  thumbsUpCount  \\\n",
       "0      sedih bgt njir, akun aku hilang ud level 40an      2              0   \n",
       "1  game bayak buk bintang 1 Update terus buk nya ...      1              1   \n",
       "2                                     baguss gamenya      5              0   \n",
       "3  ehh min lu klo ngasih tim yg ngotak apaa masa ...      1              0   \n",
       "4                              bagus banget game nya      5              0   \n",
       "\n",
       "  reviewCreatedVersion                   at replyContent repliedAt appVersion  \n",
       "0              1.109.1  2025-05-20 21:30:40          NaN       NaN    1.109.1  \n",
       "1              1.109.1  2025-05-20 21:29:45          NaN       NaN    1.109.1  \n",
       "2                  NaN  2025-05-20 21:22:48          NaN       NaN        NaN  \n",
       "3              1.109.1  2025-05-20 21:22:35          NaN       NaN    1.109.1  \n",
       "4                  NaN  2025-05-20 21:19:35          NaN       NaN        NaN  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Membaca dataset\n",
    "df = pd.read_csv('ff_scraping.csv')\n",
    "\n",
    "# Menampilkan nya\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 11 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   reviewId              10000 non-null  object\n",
      " 1   userName              10000 non-null  object\n",
      " 2   userImage             10000 non-null  object\n",
      " 3   content               10000 non-null  object\n",
      " 4   score                 10000 non-null  int64 \n",
      " 5   thumbsUpCount         10000 non-null  int64 \n",
      " 6   reviewCreatedVersion  6508 non-null   object\n",
      " 7   at                    10000 non-null  object\n",
      " 8   replyContent          7 non-null      object\n",
      " 9   repliedAt             7 non-null      object\n",
      " 10  appVersion            6508 non-null   object\n",
      "dtypes: int64(2), object(9)\n",
      "memory usage: 859.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# Menampilkan informasi dari dataset \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bisa kita lihat ada `Missing Values` pada fitur **reviewCreatedVersion**, **replyContent**, **repliedAt**, dan juga **appVersion**. \n",
    "- Disini aku bakal mengambil fitur **Score** dan juga **Content** saja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Mengambil fitur yang akan digunakan saja**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_clean = df[['score', 'content']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Penanganan Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keseluruhan Missing Values dari dataset ff_clean ini adalah 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Jumlah Missing Values</th>\n",
       "      <th>Persentase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>content</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Jumlah Missing Values  Persentase\n",
       "score                        0         0.0\n",
       "content                      0         0.0"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Menampilkan Fitur yang missing values\n",
    "missing_values = ff_clean.isnull().sum()\n",
    "missing_values_percentage = (missing_values / len(ff_clean)) * 100\n",
    "missing_values_ff_clean = pd.DataFrame({'Jumlah Missing Values': missing_values, 'Persentase': missing_values_percentage})\n",
    "print(f'Keseluruhan Missing Values dari dataset ff_clean ini adalah {ff_clean.isnull().sum().sum()}')\n",
    "\n",
    "# Cek hasil\n",
    "missing_values_ff_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Soleh Juga gak ada missing values nya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Penanganan Duplikasi Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data yang terkena duplikasi data adalah : 1776\n"
     ]
    }
   ],
   "source": [
    "print(f'Jumlah data yang terkena duplikasi data adalah : {ff_clean.duplicated().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Disini akan aku hapus saja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data yang terkena duplikasi data adalah : 0\n"
     ]
    }
   ],
   "source": [
    "ff_clean.drop_duplicates(inplace=True)\n",
    "print(f'Jumlah data yang terkena duplikasi data adalah : {ff_clean.duplicated().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preprocessing Text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cleaning Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 1. Cleaning Text ------\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Fungsi untuk membersihkan teks dari karakter karakter yang tidak di inginkan seperti angka, simbol, emoji, url, karakter khusus dll.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'@\\w+', '', text) # Menghapus mention\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text) # Menghapus hashtag\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Menghapus URL\n",
    "    text = re.sub(r'\\bRT\\b', '', text) # Menghapus RT\n",
    "    text = re.sub(r'\\d+', '', text) # Menghapus angka\n",
    "    text = re.sub(r'\\s+', ' ', text) # Menghapus whitespace berlebih\n",
    "    \n",
    "    text = text.replace('\\n', ' ') # mengganti baris baru dengan spasi\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # Menghapus tanda baca\n",
    "    text = text.strip() # Menghapus whitespace di awal dan akhir\n",
    "\n",
    "    # Menghapus emoji\n",
    "    text = emoji.replace_emoji(text, replace='')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Langkah awal untuk `Preprocessing Text` adalah dengan cara membersihkan text dari karakter karakter yang tidak di inginkan seperti angka, emoji, url, simbol, karakter khusus, tanda baca, dll. Makanya itu aku membuat fungsi **`clean_text`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Case Folding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 2. Case Folding -----\n",
    "def case_folding(text):\n",
    "    \"\"\"\n",
    "    Fungsi untuk mengubah teks menjadi huruf kecil\n",
    "    \"\"\"\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Langkah keduanya yaitu membuat text atau kalimat kalimat menjadi huruf kecil, karena dalam NLP ini di haruskan banget seragam, maka dari itu aku membuat fungsi **`case_folding`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Slang Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 3. Normalisasi Slang Words -----\n",
    "def normalize_slang(text):\n",
    "    \"\"\"\n",
    "    Fungsi untuk menormalkan kata kata slang/informal menjadi kata baku\n",
    "    \"\"\"\n",
    "    # Kamus slang words untuk Free Fire\n",
    "    slang_dict = {\n",
    "        # Game-specific\n",
    "        'ff': 'free fire', 'epep': 'free fire', 'dm': 'diamond', 'daimen': 'diamond', 'daymen': 'diamond', 'daimon': 'diamond', 'gem': 'game', 'geme': 'game', 'gm': 'game', 'boyah': 'menang', 'mabar': 'main bareng', 'cs': 'clash squad', 'br': 'battle royale', 'citer': 'cheater', 'chiter': 'cheater', 'bewan': 'bermain', 'bundel': 'bundle', 'bandle': 'bundle', 'sg': 'shotgun', 'rank': 'peringkat', 'reng': 'rank', 'renk': 'rank', 'hedsot': 'headshot','hetsot': 'headshot', 'glowal': 'gloo wall', 'gllowal': 'gloo  wall', 'bag': 'bug', 'buk': 'bug', 'afk': 'meninggalkan permainan', 'gg' : 'good game', 'ggwp': 'good game well played', 'gege': 'good game', 'setep': 'step', 'sensi': 'sensitivitas kontrol', 'bot': 'pemain yang tidak handal', 'apdet': 'update', 'apdetan': 'update', 'even': 'event', 'top up': 'isi ulang diamong', 'gameplay': 'cara bermain', 'win': 'menang', 'lose': 'kalah', 'los': 'kalah', 'reload': 'isi ulang peluru', 'damage': 'kerusakan', 'kentang': 'handphone jelek', 'demek': 'damage',\n",
    "\n",
    "        # Singkatan umum dan juga typo\n",
    "        'gw': 'saya', 'gue': 'saya', 'aku': 'saya', 'lu': 'kamu', 'lo': 'kamu', 'aja': 'saja', 'aj': 'saja', 'kalo': 'kalau', 'klo': 'kalau', 'klu': 'kalau',  'bgt': 'banget', 'yg': 'yang', 'ga': 'tidak', 'gak': 'tidak', 'gk': 'tidak', 'ngga': 'tidak', 'nggak': 'tidak', 'udah': 'sudah', 'udh': 'sudah','dah': 'sudah', 'nih': 'ini', 'emg': 'memang', 'emang': 'memang', 'kyk': 'seperti', 'kyak': 'seperti', 'kek': 'seperti', 'dgn': 'dengan', 'sm': 'sama', 'tp': 'tapi', 'tpi': 'tapi', 'pake': 'pakai', 'pakek': 'pakai','ngelag': 'lag', 'nglag': 'lag', 'leg': 'lag', 'burik': 'grafiknya jelek', 'bgus': 'bagus', 'bgs': 'bagus', 'baguss': 'bagus', 'mantep': 'mantap', 'mantap': 'sangat bagus', 'keren': 'bagus', 'seru': 'menyenangkan', 'yah': 'ya', 'krn': 'karena', 'min': 'admin', 'mohon': 'minta', 'plis': 'tolong', 'pliss': 'tolong', 'tlg': 'tolong', 'jgn': 'jangan', 'bocil': 'anak kecil', 'ud': 'sudah', 'maen': 'main', 'ngasih': 'memberi', 'ngotak': 'masuk akal', 'mulu': 'terus', 'stuk': 'terjebak', 'ganiat': 'tidak niat', 'gausah': 'tidak usah', 'di bukan': 'di buka', 'ngebantu': 'membantu', 'pingin': 'ingin', 'mlah': 'malah', 'dapet': 'dapat', 'mayan': 'lumayan', 'effort': 'usaha', 'berdamage': 'berdampak', 'kasih': 'beri', 'yok': 'ayo', 'yuk': 'ayo', 'comeback': 'kembali', 'apus': 'hapus', 'kayak': 'seperti', 'jir': 'aduh', 'njir': 'aduh', 'anjir': 'aduh', 'hoki': 'keberuntungan', 'smoth': 'halus', 'smooth': 'halus', 'smpai': 'sampai', 'smp': 'sampai', 'smpe': 'sampai', 'sampe': 'sampai', 'hamdeh': 'aduh', \n",
    "    }\n",
    "    \n",
    "    # Membuat kata kata menjadi lowercase\n",
    "    words = text.lower().split()\n",
    "    \n",
    "    # Normalisasi setiap kata\n",
    "    normalized_words = []\n",
    "    for word in words:\n",
    "        # Jika kata ada dalam kamus slang, ganti dengan kata bakunya\n",
    "        if word in slang_dict:\n",
    "            normalized_words.append(slang_dict[word])\n",
    "        else:\n",
    "            normalized_words.append(word)\n",
    "    \n",
    "    # Gabungkan kembali kata-kata menjadi teks\n",
    "    return ' '.join(normalized_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Langkah ketiga adalah kita harus mengganti kata kata yang informal seperti singkatan, dan bahasa gaul menjadi kata kata yang baku, dan juga pasti ada typo nah kita juga harus membersihkan dan mengganti dengan kalimat atau kata kata yang benar, makanya dari itu aku bikin sebuah fungsi yang bernama **`normalize_slang`**.\n",
    "\n",
    "> **ANNOUNCEMENT**\n",
    "\n",
    "- Tapi yang perlu garis bawahi adalah kata kata typo dan slangwords ini pasti banyak, aku hanya mencoba untuk mengganti beberapa kata kata yang umum digunakan, karena keterbatasan waktu dan aku adalah manusia tidak mungkin untuk satu satu melihat review dan mengganti kata kata yang typo atau slangwordsnya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 4. Tokenization -----\n",
    "def tokenization(text):\n",
    "    \"\"\"\n",
    "    Fungsi untuk mmemecah teks atau membagi teks menjadi daftar akata atau token\n",
    "    \"\"\"\n",
    "    token = word_tokenize(text)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Langkah ke 4 adalah kita harus memecah teks atau kalimat menjadi daftar kata atau token yang bermakna, ini langkah yang harus di lakukan karena kalau tidak kita tidak bisa melanjutkan ke langkah berikutnya. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **StopWords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 5. Stopword Removal -----\n",
    "def filtering_stopwords(tokens):\n",
    "    \"\"\"\n",
    "    Fungsi untuk mengganti kata kata umum yang kurang bermakna dari daftar token\n",
    "    \"\"\"\n",
    "    listStopWords = set(stopwords.words('indonesian'))\n",
    "    listStopwords1 = set(stopwords.words('english'))\n",
    "    custom_stopwords = [\n",
    "        'iya', 'yaa', 'gak', 'nga', 'gk', 'nya', 'na', 'sih', 'ku', 'di', 'ga', 'ya', \n",
    "        'gaa', 'loh', 'kah', 'woi', 'woii', 'woy', 'dong', 'deh', 'nih', 'sih',\n",
    "        'kok', 'kek', 'cuma', 'doang', 'banget', 'aja', 'doank', 'bgt', 'yg', 'udah',\n",
    "        'udh', 'dah', 'tuh', 'gitu', 'gini', 'emang', 'emg', 'kan', 'kalo', 'klo',\n",
    "        'yah', 'si', 'tau', 'gw', 'gue', 'ane', 'lu', 'lo', 'wkwk', 'wkwkwk', 'xixi',\n",
    "        'haha', 'hehe', 'tp', 'tapi', 'karna', 'krn', 'biar', 'ehh', 'eh'\n",
    "    ]\n",
    "\n",
    "    ff_stopwords = [\n",
    "    'ff', 'freefire', 'free', 'fire', 'garena', 'game', 'player', 'play', 'main',\n",
    "    'maen', 'mainkan', 'bermain', 'pemain', 'char', 'character', 'karakter',\n",
    "    'season', 'ranked', 'classic', 'match', 'skin', 'diamond', 'dm'\n",
    "]\n",
    "    listStopWords.update(ff_stopwords)\n",
    "    listStopWords.update(custom_stopwords)\n",
    "    listStopWords.update(listStopwords1)\n",
    "    filtered = []\n",
    "    for txt in tokens:\n",
    "        if txt not in listStopWords:\n",
    "            filtered.append(txt)\n",
    "    tokens = filtered\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pada tahapan ke 5 ini, kita harus mencari kata kata yang umum dan sering muncul dalam teks atau sutau kalimat tapi tidak bermakna dan tidak berkontribusi apapun di dalam hasil pemodelan. Contohnya kata **dan**, **yang**, **di** dll. Jadi intinya seperti ini aja deh, Menghapus kata kata yang tidak relevan agar analisis menjadi lebih fokus pada kata kata yang bermakna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 6. Stemming -----\n",
    "def stemming(tokens):\n",
    "    \"\"\"\n",
    "    Fungsi untuk mengubah daftar kata atau token menjadi bentuk dasarnya menggunakan Sastrawi karena bahasa Indonesia\n",
    "    \"\"\"\n",
    "    # Membuat objek stemmer \n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "\n",
    "    # Melakukan stemming pada setiap token dengan list comprehension\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Langkah ke 6 adalah `Stemming`, berfungsi untuk menyederhanakan kata ke bentuk dasarnya, agar model bisa memahami teks dengan lebih efisien dan akurat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Joining**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- 7. Join Tokens to Text -----\n",
    "def to_sentence(tokens):\n",
    "    \"\"\"\n",
    "    Fungsi untuk menggabungkan token menjadi kalimat\n",
    "    \"\"\"\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Langkah terakhir adalah menggabungkan daftar kata kata menjadi sebuah kalimat utuh lagi, karena tadi kan kita pecah pecah, dan terakhir digabungkan kembali."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Penerapan Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_text(dataset):\n",
    "    \"\"\"\n",
    "    Menerapkan pipeline preprocessing text pada dataset dengan analisis missing values.\n",
    "    \"\"\"\n",
    "\n",
    "    df = dataset.copy()\n",
    "    total_data = len(df)\n",
    "\n",
    "    print(f'Memulai preprocessing untuk {total_data} data ulasan Free Fire\\n')\n",
    "    \n",
    "    # Fungsi untuk memeriksa dan melaporkan missing values\n",
    "    def check_missing(df, column_name):\n",
    "        missing = df[column_name].isna().sum()\n",
    "        empty = (df[column_name].astype(str) == '').sum()\n",
    "        total_invalid = missing + empty\n",
    "        percent = (total_invalid / len(df)) * 100\n",
    "        \n",
    "        print(f'Status data: {len(df)-total_invalid} valid, {total_invalid} missing/empty ({percent:.2f}%)')\n",
    "        return total_invalid\n",
    "\n",
    "    try:\n",
    "        # 1. Cleaning Text\n",
    "        df['clean_text'] = df['content'].apply(clean_text)\n",
    "        check_missing(df, 'clean_text')\n",
    "        print(f'Proses Cleaning Text done Mamang!!!')\n",
    "\n",
    "        # 2. Case Folding\n",
    "        df['case_foldingText'] = df['clean_text'].apply(case_folding)\n",
    "        check_missing(df, 'case_foldingText')\n",
    "        print(f'Proses Case Folding done Mamang!!!')\n",
    "\n",
    "        # 3. Normalisasi Slang Words \n",
    "        df['normalized_slangwordsText'] = df['case_foldingText'].apply(normalize_slang)\n",
    "        check_missing(df, 'normalized_slangwordsText')\n",
    "        print(f'Proses Normalisasi Slang Words done Mamang!!!')\n",
    "\n",
    "        # 4. Tokenizing\n",
    "        df['tokenized_text'] = df['normalized_slangwordsText'].apply(tokenization)\n",
    "\n",
    "        # Untuk kolom list, periksa list kosong\n",
    "        tokenize_empty = sum(1 for x in df['tokenized_text'] if not x)\n",
    "        print(f'Status data: {len(df)-tokenize_empty} valid, {tokenize_empty} empty lists ({tokenize_empty/len(df)*100:.2f}%)')\n",
    "        print(f'Proses Tokenizing done Mamang!!!')\n",
    "\n",
    "        # 5. Stopword Removal\n",
    "        df['stopword_removedText'] = df['tokenized_text'].apply(filtering_stopwords)\n",
    "        stopword_empty = sum(1 for x in df['stopword_removedText'] if not x)\n",
    "        print(f'Status data: {len(df)-stopword_empty} valid, {stopword_empty} empty lists ({stopword_empty/len(df)*100:.2f}%)')\n",
    "        print(f'Proses Stopword Removal done Mamang!!!')\n",
    "\n",
    "        # 6. Stemming\n",
    "        df['stemmed_text'] = df['stopword_removedText'].apply(stemming)\n",
    "        stemming_empty = sum(1 for x in df['stemmed_text'] if not x)\n",
    "        print(f'Status data: {len(df)-stemming_empty} valid, {stemming_empty} empty lists ({stemming_empty/len(df)*100:.2f}%)')\n",
    "        print(f'Proses Stemming done Mamang!!!')\n",
    "\n",
    "        # 7. Join Tokens to Text\n",
    "        df['final_text'] = df['stemmed_text'].apply(to_sentence)\n",
    "        final_missing = check_missing(df, 'final_text')\n",
    "        print(f'Proses Join Tokens to Text done Mamang!!!')\n",
    "\n",
    "        # Ringkasan hasil preprocessing\n",
    "        print('\\nRINGKASAN HASIL PREPROCESSING:')\n",
    "        print(f'Total data awal: {total_data}')\n",
    "        print(f'Total data valid setelah preprocessing: {total_data - final_missing}')\n",
    "        print(f'Total data invalid/kosong: {final_missing} ({final_missing/total_data*100:.2f}%)')\n",
    "        \n",
    "        # Analisis data yang hilang di tahap akhir (opsional)\n",
    "        if final_missing > 0:\n",
    "            print('\\nAnalisis data kosong:')\n",
    "            empty_indices = df[df['final_text'] == ''].index\n",
    "            for idx in empty_indices[:min(3, len(empty_indices))]:\n",
    "                print(f'\\nData kosong #{idx}:')\n",
    "                print(f'Teks asli: {df[\"content\"].iloc[idx]}')\n",
    "                print(f'Setelah cleaning: {df[\"clean_text\"].iloc[idx]}')\n",
    "                \n",
    "        print(f'\\nPREPROCESSING SELESAI MAMANG!')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f'\\nWalawe Terjadi error pada preprocessing Mamang: {str(e)}')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memulai preprocessing untuk 8224 data ulasan Free Fire\n",
      "\n",
      "Status data: 8155 valid, 69 missing/empty (0.84%)\n",
      "Proses Cleaning Text done Mamang!!!\n",
      "Status data: 8155 valid, 69 missing/empty (0.84%)\n",
      "Proses Case Folding done Mamang!!!\n",
      "Status data: 8154 valid, 70 missing/empty (0.85%)\n",
      "Proses Normalisasi Slang Words done Mamang!!!\n",
      "Status data: 8154 valid, 70 empty lists (0.85%)\n",
      "Proses Tokenizing done Mamang!!!\n",
      "Status data: 8047 valid, 177 empty lists (2.15%)\n",
      "Proses Stopword Removal done Mamang!!!\n",
      "Status data: 8047 valid, 177 empty lists (2.15%)\n",
      "Proses Stemming done Mamang!!!\n",
      "Status data: 8042 valid, 182 missing/empty (2.21%)\n",
      "Proses Join Tokens to Text done Mamang!!!\n",
      "\n",
      "RINGKASAN HASIL PREPROCESSING:\n",
      "Total data awal: 8224\n",
      "Total data valid setelah preprocessing: 8042\n",
      "Total data invalid/kosong: 182 (2.21%)\n",
      "\n",
      "Analisis data kosong:\n",
      "\n",
      "Data kosong #17:\n",
      "Teks asli: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠êüòº‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠êüíõ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠êüòº‚≠ê‚≠êüòº‚≠êüåõ‚≠ê‚≠êüòº‚≠êüòº‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠êüåü‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠êüåü‚≠ê‚≠ê‚≠ê‚≠ê‚ö°‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠êüéä‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠êüíõ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠êüåù‚≠ê‚≠ê‚≠ê‚≠êüåõ‚≠ê‚≠ê‚≠êüï≥Ô∏èüåú‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n",
      "Setelah cleaning: \n",
      "\n",
      "Data kosong #23:\n",
      "Teks asli: geme gak jelas\n",
      "Setelah cleaning: geme gak jelas\n",
      "\n",
      "Data kosong #55:\n",
      "Teks asli: üëç\n",
      "Setelah cleaning: \n",
      "\n",
      "PREPROCESSING SELESAI MAMANG!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>content</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>case_foldingText</th>\n",
       "      <th>normalized_slangwordsText</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>stopword_removedText</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>final_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>sedih bgt njir, akun aku hilang ud level 40an</td>\n",
       "      <td>sedih bgt njir akun aku hilang ud level an</td>\n",
       "      <td>sedih bgt njir akun aku hilang ud level an</td>\n",
       "      <td>sedih banget aduh akun saya hilang sudah level an</td>\n",
       "      <td>[sedih, banget, aduh, akun, saya, hilang, suda...</td>\n",
       "      <td>[sedih, aduh, akun, hilang, level]</td>\n",
       "      <td>[sedih, aduh, akun, hilang, level]</td>\n",
       "      <td>sedih aduh akun hilang level</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>game bayak buk bintang 1 Update terus buk nya ...</td>\n",
       "      <td>game bayak buk bintang Update terus buk nya ng...</td>\n",
       "      <td>game bayak buk bintang update terus buk nya ng...</td>\n",
       "      <td>game bayak bug bintang update terus bug nya ti...</td>\n",
       "      <td>[game, bayak, bug, bintang, update, terus, bug...</td>\n",
       "      <td>[bayak, bug, bintang, update, bug, perbaiki, b...</td>\n",
       "      <td>[bayak, bug, bintang, update, bug, baik, bug, ...</td>\n",
       "      <td>bayak bug bintang update bug baik bug damage p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>baguss gamenya</td>\n",
       "      <td>baguss gamenya</td>\n",
       "      <td>baguss gamenya</td>\n",
       "      <td>bagus gamenya</td>\n",
       "      <td>[bagus, gamenya]</td>\n",
       "      <td>[bagus, gamenya]</td>\n",
       "      <td>[bagus, gamenya]</td>\n",
       "      <td>bagus gamenya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>ehh min lu klo ngasih tim yg ngotak apaa masa ...</td>\n",
       "      <td>ehh min lu klo ngasih tim yg ngotak apaa masa ...</td>\n",
       "      <td>ehh min lu klo ngasih tim yg ngotak apaa masa ...</td>\n",
       "      <td>ehh admin kamu kalau memberi tim yang masuk ak...</td>\n",
       "      <td>[ehh, admin, kamu, kalau, memberi, tim, yang, ...</td>\n",
       "      <td>[ehh, admin, tim, masuk, akal, apaa, clash, sq...</td>\n",
       "      <td>[ehh, admin, tim, masuk, akal, apaa, clash, sq...</td>\n",
       "      <td>ehh admin tim masuk akal apaa clash squad elit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>bagus banget game nya</td>\n",
       "      <td>bagus banget game nya</td>\n",
       "      <td>bagus banget game nya</td>\n",
       "      <td>bagus banget game nya</td>\n",
       "      <td>[bagus, banget, game, nya]</td>\n",
       "      <td>[bagus]</td>\n",
       "      <td>[bagus]</td>\n",
       "      <td>bagus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                            content  \\\n",
       "0      2      sedih bgt njir, akun aku hilang ud level 40an   \n",
       "1      1  game bayak buk bintang 1 Update terus buk nya ...   \n",
       "2      5                                     baguss gamenya   \n",
       "3      1  ehh min lu klo ngasih tim yg ngotak apaa masa ...   \n",
       "4      5                              bagus banget game nya   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0         sedih bgt njir akun aku hilang ud level an   \n",
       "1  game bayak buk bintang Update terus buk nya ng...   \n",
       "2                                     baguss gamenya   \n",
       "3  ehh min lu klo ngasih tim yg ngotak apaa masa ...   \n",
       "4                              bagus banget game nya   \n",
       "\n",
       "                                    case_foldingText  \\\n",
       "0         sedih bgt njir akun aku hilang ud level an   \n",
       "1  game bayak buk bintang update terus buk nya ng...   \n",
       "2                                     baguss gamenya   \n",
       "3  ehh min lu klo ngasih tim yg ngotak apaa masa ...   \n",
       "4                              bagus banget game nya   \n",
       "\n",
       "                           normalized_slangwordsText  \\\n",
       "0  sedih banget aduh akun saya hilang sudah level an   \n",
       "1  game bayak bug bintang update terus bug nya ti...   \n",
       "2                                      bagus gamenya   \n",
       "3  ehh admin kamu kalau memberi tim yang masuk ak...   \n",
       "4                              bagus banget game nya   \n",
       "\n",
       "                                      tokenized_text  \\\n",
       "0  [sedih, banget, aduh, akun, saya, hilang, suda...   \n",
       "1  [game, bayak, bug, bintang, update, terus, bug...   \n",
       "2                                   [bagus, gamenya]   \n",
       "3  [ehh, admin, kamu, kalau, memberi, tim, yang, ...   \n",
       "4                         [bagus, banget, game, nya]   \n",
       "\n",
       "                                stopword_removedText  \\\n",
       "0                 [sedih, aduh, akun, hilang, level]   \n",
       "1  [bayak, bug, bintang, update, bug, perbaiki, b...   \n",
       "2                                   [bagus, gamenya]   \n",
       "3  [ehh, admin, tim, masuk, akal, apaa, clash, sq...   \n",
       "4                                            [bagus]   \n",
       "\n",
       "                                        stemmed_text  \\\n",
       "0                 [sedih, aduh, akun, hilang, level]   \n",
       "1  [bayak, bug, bintang, update, bug, baik, bug, ...   \n",
       "2                                   [bagus, gamenya]   \n",
       "3  [ehh, admin, tim, masuk, akal, apaa, clash, sq...   \n",
       "4                                            [bagus]   \n",
       "\n",
       "                                          final_text  \n",
       "0                       sedih aduh akun hilang level  \n",
       "1  bayak bug bintang update bug baik bug damage p...  \n",
       "2                                      bagus gamenya  \n",
       "3  ehh admin tim masuk akal apaa clash squad elit...  \n",
       "4                                              bagus  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implementasi Function\n",
    "process_df = preprocessing_text(ff_clean)\n",
    "process_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dan alhamdulillah hasilnya lancar dan aman ya, tapi masalahnya disini ada missing values nih setelah kita melakukan preprocessing kepada text, kemungkinan ini adalah emoji emoji yang di replace menjadi nilai yang kosong, maka dari itu kita akan menghapusnya saja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
